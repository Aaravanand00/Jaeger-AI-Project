# AI Backend with Multi-Provider LLM Support

## Architecture Overview: Cloud and Local LLM Integration

```mermaid
graph TB
    %% Frontend Layer
    subgraph "üñ•Ô∏è Frontend"
        UI[Jaeger React UI<br/>Natural Language Interface]
    end
    
    %% AI Backend Layer
    subgraph "ü§ñ AI Backend Service"
        API[REST API Layer<br/>Query & Explain Endpoints]
        BUSINESS[Business Logic<br/>Query Translation & Span Analysis]
    end
    
    %% LLM Abstraction Layer
    subgraph "üîå LLM Abstraction Layer"
        INTERFACE[LLM Client Interface<br/>Unified API Contract]
        FACTORY[Provider Factory<br/>Selects Implementation]
    end
    
    %% Configuration
    subgraph "‚öôÔ∏è Configuration"
        CONFIG[config.yaml<br/>Provider Selection]
        ENV[Environment Variables<br/>API Keys & Settings]
    end
    
    %% LLM Providers
    subgraph "üß† LLM Providers"
        subgraph "üè† Local LLM (Privacy-First)"
            OLLAMA[Ollama Server<br/>localhost:11434]
            MODELS[Local Models<br/>llama3, codellama, mistral]
        end
        
        subgraph "‚òÅÔ∏è Cloud LLMs (Optional)"
            OPENAI[OpenAI GPT-4<br/>API Client]
            ANTHROPIC[Anthropic Claude<br/>API Client]
        end
        
        MOCK[Mock Provider<br/>Development & Testing]
    end

    %% Request Flow
    UI -->|HTTP Requests Only| API
    API --> BUSINESS
    BUSINESS -->|Uses Abstraction| INTERFACE
    INTERFACE --> FACTORY
    
    %% Provider Selection (Configuration-Driven)
    CONFIG -->|provider: "ollama"| FACTORY
    CONFIG -->|provider: "openai"| FACTORY
    CONFIG -->|provider: "anthropic"| FACTORY
    ENV -->|API Keys| FACTORY
    
    %% Provider Implementations
    FACTORY -->|Local Deployment| OLLAMA
    FACTORY -.->|Cloud Option| OPENAI
    FACTORY -.->|Cloud Option| ANTHROPIC
    FACTORY -.->|Dev/Test| MOCK
    
    %% Local LLM Details
    OLLAMA --> MODELS

    %% Styling
    classDef frontend fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef backend fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef abstraction fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef config fill:#f1f8e9,stroke:#689f38,stroke-width:2px
    classDef local fill:#e0f2f1,stroke:#00796b,stroke-width:3px
    classDef cloud fill:#fff8e1,stroke:#fbc02d,stroke-width:2px
    classDef mock fill:#fce4ec,stroke:#c2185b,stroke-width:2px

    class UI frontend
    class API,BUSINESS backend
    class INTERFACE,FACTORY abstraction
    class CONFIG,ENV config
    class OLLAMA,MODELS local
    class OPENAI,ANTHROPIC cloud
    class MOCK mock
```

## Key Architecture Benefits

### üîí **Clean Separation of Concerns**

#### Business Logic Layer
```go
// Business logic never knows which LLM is being used
type QueryService struct {
    llmClient LLMClient  // Interface, not concrete implementation
}

func (s *QueryService) TranslateQuery(query string) (*SearchParams, error) {
    // Business logic focuses on the problem, not the LLM provider
    response, err := s.llmClient.Complete(CompletionRequest{
        Prompt: buildQueryPrompt(query),
        Temperature: 0.0,
    })
    // ... process response
}
```

#### LLM Abstraction Interface
```go
type LLMClient interface {
    Complete(request CompletionRequest) (CompletionResponse, error)
    IsConfigured() bool
    GetProvider() string
}
```

### üè† **Privacy-First Local Deployment**

#### Ollama Integration
```yaml
# config.yaml - Local LLM Configuration
llm:
  provider: "ollama"
  ollama:
    endpoint: "http://localhost:11434"
    model: "llama3"
    timeout: "30s"
    
# Benefits:
# ‚úÖ Data never leaves your infrastructure
# ‚úÖ No API costs
# ‚úÖ No internet dependency
# ‚úÖ Full control over model versions
```

#### Local Setup Commands
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Download and run a model
ollama pull llama3
ollama serve

# AI Backend automatically connects
```

### ‚òÅÔ∏è **Optional Cloud Providers**

#### OpenAI Configuration
```yaml
# config.yaml - Cloud LLM Configuration
llm:
  provider: "openai"
  openai:
    model: "gpt-4"
    temperature: 0.0
    max_tokens: 500
    
# Environment variables
OPENAI_API_KEY=sk-...
```

#### Anthropic Configuration
```yaml
# config.yaml - Alternative Cloud Provider
llm:
  provider: "anthropic"
  anthropic:
    model: "claude-3-sonnet"
    temperature: 0.0
    max_tokens: 500
    
# Environment variables
ANTHROPIC_API_KEY=sk-ant-...
```

## Provider Selection Logic

### Configuration-Driven Selection
```go
func NewLLMClient(config Config) (LLMClient, error) {
    switch config.LLM.Provider {
    case "ollama":
        return NewOllamaClient(config.LLM.Ollama)
    case "openai":
        return NewOpenAIClient(config.LLM.OpenAI)
    case "anthropic":
        return NewAnthropicClient(config.LLM.Anthropic)
    case "mock":
        return NewMockClient()
    default:
        return nil, fmt.Errorf("unknown provider: %s", config.LLM.Provider)
    }
}
```

### Fallback Strategy
```go
// Graceful degradation
func NewLLMClientWithFallback(config Config) LLMClient {
    // Try primary provider
    if client, err := NewLLMClient(config); err == nil && client.IsConfigured() {
        return client
    }
    
    // Fallback to mock for development
    log.Warn("Primary LLM provider unavailable, using mock client")
    return NewMockClient()
}
```

## Deployment Scenarios

### üè¢ **Enterprise Local Deployment**
```yaml
# Maximum privacy and control
llm:
  provider: "ollama"
  ollama:
    endpoint: "http://ollama-server:11434"
    model: "llama3"

# Benefits:
# - All data stays within corporate network
# - No external API dependencies
# - Predictable costs (hardware only)
# - Custom model fine-tuning possible
```

### ‚òÅÔ∏è **Cloud-First Deployment**
```yaml
# Maximum convenience and scalability
llm:
  provider: "openai"
  openai:
    model: "gpt-4"

# Benefits:
# - No local infrastructure required
# - Latest model capabilities
# - Unlimited scalability
# - Managed service reliability
```

### üîÑ **Hybrid Deployment**
```yaml
# Primary: Local for privacy
# Fallback: Cloud for availability
llm:
  provider: "ollama"
  fallback_provider: "openai"
  
# Use cases:
# - Local for sensitive data
# - Cloud for high-volume periods
# - Development flexibility
```

## Request Flow Examples

### Local LLM Flow
```
1. User: "show slow payments"
2. Jaeger UI ‚Üí AI Backend
3. Backend ‚Üí LLM Abstraction
4. Abstraction ‚Üí Ollama (localhost:11434)
5. Ollama ‚Üí llama3 model
6. Response: {"service": "payment", "minDuration": "500ms"}
7. Backend ‚Üí Jaeger Query Service
8. Results ‚Üí User
```

### Cloud LLM Flow
```
1. User: "show slow payments"
2. Jaeger UI ‚Üí AI Backend
3. Backend ‚Üí LLM Abstraction
4. Abstraction ‚Üí OpenAI API
5. OpenAI ‚Üí GPT-4 model
6. Response: {"service": "payment", "minDuration": "500ms"}
7. Backend ‚Üí Jaeger Query Service
8. Results ‚Üí User
```

## Configuration Examples

### Complete Local Configuration
```yaml
# config.yaml
server:
  port: 8080
  
llm:
  provider: "ollama"
  ollama:
    endpoint: "http://localhost:11434"
    model: "llama3"
    timeout: "30s"
    
jaeger:
  query_endpoint: "http://localhost:16686"
  
logging:
  level: "info"
```

### Complete Cloud Configuration
```yaml
# config.yaml
server:
  port: 8080
  
llm:
  provider: "openai"
  openai:
    model: "gpt-4"
    temperature: 0.0
    max_tokens: 500
    timeout: "10s"
    
jaeger:
  query_endpoint: "http://localhost:16686"
  
logging:
  level: "info"
```

### Environment Variables
```bash
# .env file
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
LOG_LEVEL=info
JAEGER_QUERY_ENDPOINT=http://jaeger:16686
```

## Provider Comparison

| Feature | **Local (Ollama)** | **Cloud (OpenAI/Anthropic)** |
|---------|-------------------|-------------------------------|
| **Privacy** | ‚úÖ Complete data privacy | ‚ùå Data sent to external API |
| **Cost** | ‚úÖ Free after setup | ‚ùå Pay per request |
| **Setup** | ‚ö†Ô∏è Requires local installation | ‚úÖ Just API key needed |
| **Performance** | ‚ö†Ô∏è Depends on hardware | ‚úÖ Consistent high performance |
| **Availability** | ‚ö†Ô∏è Depends on local infrastructure | ‚úÖ Managed service uptime |
| **Customization** | ‚úÖ Can fine-tune models | ‚ùå Limited to provided models |
| **Scalability** | ‚ö†Ô∏è Limited by hardware | ‚úÖ Unlimited scaling |
| **Internet** | ‚úÖ Works offline | ‚ùå Requires internet connection |

## Implementation Benefits

### üîß **Easy Provider Switching**
```bash
# Switch from cloud to local
sed -i 's/provider: "openai"/provider: "ollama"/' config.yaml

# Restart service - no code changes needed
systemctl restart jaeger-ai-backend
```

### üß™ **Development Flexibility**
```yaml
# Development environment
llm:
  provider: "mock"  # Fast, predictable responses

# Staging environment  
llm:
  provider: "ollama"  # Test with real AI, no costs

# Production environment
llm:
  provider: "openai"  # Maximum reliability
```

### üîí **Security Compliance**
```yaml
# For regulated industries
llm:
  provider: "ollama"  # Data never leaves premises
  
# For general use
llm:
  provider: "openai"  # Leverage managed service
```

---

**This architecture provides maximum flexibility while maintaining clean separation of concerns, enabling teams to choose the LLM deployment strategy that best fits their privacy, cost, and performance requirements.**