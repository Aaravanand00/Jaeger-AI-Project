# AI-Powered Trace Analysis in Jaeger - Architecture Overview

## Simple Architecture Flowchart

```mermaid
graph TB
    %% User and Frontend
    USER[ğŸ‘¤ User<br/>Types: "show slow payments"]
    
    %% Layer 1: Frontend
    subgraph "ğŸ–¥ï¸ Frontend Layer"
        UI[Jaeger React UI<br/>Web Interface]
    end
    
    %% Layer 2: AI Backend
    subgraph "ğŸ¤– AI Backend Service (Go + LangChainGo)"
        BACKEND[AI Backend<br/>Processes Requests]
    end
    
    %% Layer 3: LLM Abstraction
    subgraph "ğŸ”Œ LLM Abstraction Layer"
        ABSTRACTION[LLM Interface<br/>Chooses AI Provider]
    end
    
    %% Layer 4: LLM Providers
    subgraph "ğŸ§  AI Providers"
        subgraph "â˜ï¸ Cloud LLMs"
            OPENAI[OpenAI GPT-4]
            CLAUDE[Anthropic Claude]
        end
        
        subgraph "ğŸ  Local LLM (Private)"
            OLLAMA[Ollama Server<br/>Runs on Your Machine]
            MODELS[Local AI Models<br/>llama3, codellama]
        end
    end
    
    %% Layer 5: Jaeger Services
    subgraph "ğŸ” Jaeger Query Service"
        JAEGER[Jaeger API<br/>Searches Traces]
        STORAGE[(Trace Database<br/>Your Trace Data)]
    end

    %% Main Flow - Natural Language Query
    USER -->|1. "show slow payments"| UI
    UI -->|2. Send query| BACKEND
    BACKEND -->|3. Ask AI to interpret| ABSTRACTION
    
    %% AI Provider Selection (Configurable)
    ABSTRACTION -.->|Option A: Cloud| OPENAI
    ABSTRACTION -.->|Option A: Cloud| CLAUDE
    ABSTRACTION -->|Option B: Local<br/>(Private & Free)| OLLAMA
    OLLAMA --> MODELS
    
    %% Response Flow
    ABSTRACTION -->|4. AI Response:<br/>service=payment<br/>minDuration=500ms| BACKEND
    BACKEND -->|5. Search with<br/>structured params| JAEGER
    JAEGER -->|6. Query database| STORAGE
    STORAGE -->|7. Return traces| JAEGER
    JAEGER -->|8. Trace results| BACKEND
    BACKEND -->|9. Final results| UI
    UI -->|10. Show traces| USER

    %% Configuration
    CONFIG[âš™ï¸ Configuration<br/>Choose: Local or Cloud AI]
    CONFIG -.->|Controls which AI to use| ABSTRACTION

    %% Styling for clarity
    classDef user fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef frontend fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef backend fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef abstraction fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef cloud fill:#fff8e1,stroke:#fbc02d,stroke-width:2px
    classDef local fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    classDef jaeger fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef config fill:#f1f8e9,stroke:#689f38,stroke-width:2px

    class USER user
    class UI frontend
    class BACKEND backend
    class ABSTRACTION abstraction
    class OPENAI,CLAUDE cloud
    class OLLAMA,MODELS local
    class JAEGER,STORAGE jaeger
    class CONFIG config
```

## How It Works (Step by Step)

### ğŸ”„ Natural Language Query Flow

```
ğŸ‘¤ User Types: "show me slow payment requests"
    â†“
ğŸ–¥ï¸ Jaeger UI: Sends query to AI backend
    â†“
ğŸ¤– AI Backend: "I need to understand this query"
    â†“
ğŸ”Œ LLM Abstraction: Chooses AI provider (Local or Cloud)
    â†“
ğŸ§  AI Provider: Interprets query â†’ Returns structured search
    â†“
ğŸ¤– AI Backend: Gets structured params like:
   {
     "service": "payment",
     "minDuration": "500ms",
     "tags": {"http.status_code": "200"}
   }
    â†“
ğŸ” Jaeger Query: Searches traces with these parameters
    â†“
ğŸ–¥ï¸ Jaeger UI: Shows filtered trace results to user
```

### ğŸ  Local LLM vs â˜ï¸ Cloud LLM

| **Local LLM (Ollama)** | **Cloud LLM (OpenAI/Claude)** |
|-------------------------|--------------------------------|
| âœ… **Private**: Data stays on your machine | âŒ **Public**: Data sent to external API |
| âœ… **Free**: No API costs | âŒ **Paid**: Costs per request |
| âœ… **Fast**: No network latency | âš ï¸ **Network**: Depends on internet |
| âš ï¸ **Setup**: Requires local installation | âœ… **Easy**: Just need API key |
| âš ï¸ **Resources**: Uses your CPU/GPU | âœ… **Scalable**: Unlimited capacity |

### âš™ï¸ Configuration Example

**Choose Local LLM:**
```yaml
# config.yaml
llm:
  provider: "ollama"    # Use local AI
  model: "llama3"       # Which local model
```

**Choose Cloud LLM:**
```yaml
# config.yaml
llm:
  provider: "openai"    # Use cloud AI
  model: "gpt-4"        # Which cloud model
```

## Real-World Example

### Input: Natural Language
```
User types: "find slow database calls in the checkout service from the last hour"
```

### AI Processing
The AI understands this means:
- **Service**: `checkout`
- **Operation**: `database` or `db`
- **Performance**: `slow` (duration > 500ms)
- **Time Range**: `last hour`

### Output: Structured Search
```json
{
  "service": "checkout",
  "operation": "db",
  "minDuration": "500ms",
  "lookback": "1h",
  "tags": {
    "span.kind": "client",
    "db.type": "*"
  }
}
```

### Result: Filtered Traces
Jaeger returns only the traces that match these exact criteria, making it easy to find the slow database calls you're looking for.

## Why This Architecture?

### ğŸ¯ **Simple for Users**
- Type questions in plain English
- No need to learn Jaeger query syntax
- Get exactly what you're looking for

### ğŸ”§ **Flexible for Teams**
- Choose local AI for privacy
- Choose cloud AI for convenience
- Switch between providers easily

### ğŸ—ï¸ **Clean Architecture**
- Each layer has one job
- Easy to test and maintain
- Can swap components independently

### ğŸš€ **Production Ready**
- Handles errors gracefully
- Scales with your team
- Integrates with existing Jaeger setup

---

**This architecture makes distributed tracing accessible to everyone on your team, regardless of their Jaeger expertise level.**